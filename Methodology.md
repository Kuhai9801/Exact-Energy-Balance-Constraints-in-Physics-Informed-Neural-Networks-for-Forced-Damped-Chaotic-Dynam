# Methodology

## Problem Setting and Objective
This project investigates physics-informed neural networks (PINNs) for learning chaotic dynamics in open nonlinear systems from sparse, noisy, and partially observed data. The forced–damped Duffing oscillator is used as a canonical benchmark because it combines strong nonlinearity, external forcing, dissipation, and sensitive dependence on initial conditions.

The methodological objective is to isolate the effect of auxiliary energy constraints on attractor recovery. To achieve this, the study compares a standard PINN, a PINN with an intentionally mismatched energy-conservation regularization, and a PINN with a physics-consistent energy-balance constraint. All models are evaluated under identical architectures, optimization protocols, sampling strategies, and random seeds to ensure that observed differences arise strictly from the imposed constraints rather than implementation artifacts.

## Governing Dynamics
The forced–damped Duffing oscillator is governed by the second-order ordinary differential equation:

$$
\ddot{x}(t) + \delta \dot{x}(t) + \alpha x(t) + \beta x(t)^3 = \gamma \cos(\omega t)
$$

where $x(t)$ denotes displacement and $\alpha, \beta, \delta, \gamma, \omega$ are fixed system parameters. For learning and constraint enforcement, the system is expressed in first-order state-space form by introducing the velocity $v(t) = \dot{x}(t)$:

$$
\begin{aligned}
\dot{x}(t) &= v(t) \\
\dot{v}(t) &= -\delta v(t) - \alpha x(t) - \beta x(t)^3 + \gamma \cos(\omega t)
\end{aligned}
$$

The forcing period is defined as $T = 2\pi / \omega$. A single parameter regime is selected to exhibit sustained chaotic behavior.

## Energy Function and Exact Balance Law
A mechanical energy function for the system is defined as:

$$
H(x, v) = \frac{1}{2} v^2 + \frac{1}{2} \alpha x^2 + \frac{1}{4} \beta x^4
$$

Because the system is forced and damped, total energy is not conserved. Instead, the dynamics satisfy an exact power balance relation:

$$
\frac{dH}{dt} = \gamma v \cos(\omega t) - \delta v^2
$$

The first term on the right-hand side represents instantaneous power input from the external forcing, while the second term represents viscous dissipation. This balance law provides a physically correct auxiliary constraint for open systems and serves as a valid alternative to inappropriate energy-conservation assumptions.

## Reference Trajectory Generation
Ground-truth trajectories are generated by numerically integrating the Duffing system using a high-order Runge-Kutta solver (RK45) with strict relative and absolute tolerances ($10^{-9}$) to ensure reliable long-time statistics. The resulting dense trajectory is stored as $(t, x_{true}(t), v_{true}(t))$. An initial transient interval is removed, and all attractor-level evaluations are performed on the remaining stationary regime.

## Observation Model and Dataset Construction
Learning is performed under partial observation. Only position measurements are available and are generated by adding Gaussian noise to the ground truth:

$$
x_{obs}(t_j) = x_{true}(t_j) + \epsilon_j, \quad \epsilon_j \sim \mathcal{N}(0, \sigma_x^2)
$$

The observed series is sparsified by retaining every $k$-th sample. To emulate sensor dropout, contiguous time windows are removed by setting observations to NaN. A binary mask $m_j \in \{0, 1\}$ is retained to indicate measurement availability. The set of valid measurement timestamps is denoted as $T_{data}$, and the set of collocation timestamps sampled uniformly over the training interval is denoted as $T_{col}$. Train, validation, and test splits are defined as contiguous time blocks to prevent temporal leakage.

## Baselines and Learned Representations
A purely data-driven baseline is included to quantify the benefit of physics constraints under sparse and noisy observation. The baseline is an autoregressive multilayer perceptron trained only on observed position, augmented with the known forcing phase components $\cos(\omega t)$ and $\sin(\omega t)$. The model is rolled out iteratively to generate long trajectories for attractor evaluation.

For the physics-informed models, a neural network parameterizes the full state as a function of time, denoted as $(x_\theta(t), v_\theta(t)) = \mathcal{NN}_\theta(t)$. Automatic differentiation is used to compute time derivatives, enabling residual-based enforcement of the governing equations.

## Collocation Sampling and Physics Residuals
The physics residuals enforced at collocation points $t \in T_{col}$ are defined as:

$$
\begin{aligned}
r_1(t) &= \frac{dx_\theta}{dt} - v_\theta(t) \\
r_2(t) &= \frac{dv_\theta}{dt} + \delta v_\theta(t) + \alpha x_\theta(t) + \beta x_\theta(t)^3 - \gamma \cos(\omega t)
\end{aligned}
$$

Collocation density is fixed across all model variants, with limited robustness checks performed to verify insensitivity to discretization.

## Loss Functions and Model Variants
The masked data loss enforces agreement with observed position on $T_{data}$:

$$
L_{data} = \frac{1}{|T_{data}|} \sum_{t_j \in T_{data}} m_j (x_\theta(t_j) - x_{obs}(t_j))^2
$$

The initial condition loss enforces the starting state at time $t_0$:

$$
L_{ic} = (x_\theta(t_0) - x_0)^2 + (v_\theta(t_0) - v_0)^2
$$

The physics loss enforces the governing equations at the collocation points $T_{col}$:

$$
L_{phys} = \frac{1}{|T_{col}|} \sum_{t_i \in T_{col}} \left( r_1(t_i)^2 + r_2(t_i)^2 \right)
$$

Three PINN variants are evaluated to isolate the impact of physical constraints. The first model, **M0** (Standard PINN), enforces only the governing differential equations via the loss function:

$$
L(M0) = \lambda_{data} L_{data} + \lambda_{ic} L_{ic} + \lambda_{phys} L_{phys}
$$

The second model, **M1** (Conservation Regularization), augments the standard objective with an energy-conservation penalty that enforces $H_\theta(t) \approx H_\theta(t_0)$. This formulation is intentionally inconsistent with forced–damped dynamics and is used to expose failure modes induced by false invariants. The conservation loss is given by:

$$
L_{Econs} = \frac{1}{|T_{col}|} \sum_{t_i \in T_{col}} (H_\theta(t_i) - H_\theta(t_0))^2
$$

The third model, **M2** (Balance Constraint), replaces conservation with the physically correct energy-balance constraint. The balance residual is defined as:

$$
r_E(t) = \frac{dH_\theta}{dt} - (\gamma v_\theta(t) \cos(\omega t) - \delta v_\theta(t)^2)
$$

The corresponding balance loss is formulated as:

$$
L_{Ebal} = \frac{1}{|T_{col}|} \sum_{t_i \in T_{col}} r_E(t_i)^2
$$

The total objective function for M2 is:

$$
L(M2) = L(M0) + \lambda_E L_{Ebal}
$$

## Scaling and Loss Balancing Protocol
To ensure stable optimization and avoid scale-induced bias, state variables and energy residuals are normalized using only observable quantities and known system parameters. Position is standardized using its sample standard deviation. Velocity is estimated from noisy position measurements using finite differences (e.g., central difference) on available points:

$$
\hat{v}(t_j) \approx \frac{x_{obs}(t_{j+1}) - x_{obs}(t_{j-1})}{t_{j+1} - t_{j-1}}
$$

The energy-balance residual is normalized by an empirical characteristic power scale computed from this estimated velocity:

$$
s_{dH} = \text{median}_j \left| \gamma \hat{v}(t_j) \cos(\omega t_j) - \delta \hat{v}(t_j)^2 \right| + \epsilon
$$

This procedure avoids information leakage from the reference trajectory. A minimal sweep of the energy-balance weight $\lambda_E$ is reported, while all other loss weights are held fixed.

## Optimization and Reproducibility
Training is performed using the Adam optimizer with a learning-rate schedule. Identical architectures, initialization seeds, collocation sampling procedures, and optimization settings are used across all models. Model selection is based on validation attractor metrics rather than long-horizon trajectory error. To ensure reproducibility, all experiments explicitly log the random seed, parameter regime, sampling rates, noise levels, dropout window specification, number of collocation points ($N_{col}$), optimizer settings, and all loss weights.

## Evaluation Protocol for Chaotic Dynamics
Evaluation prioritizes attractor-level fidelity rather than long-horizon trajectory matching. 

1. Poincaré Sections are constructed by sampling predicted and reference trajectories once per forcing period at times $t_k = t_{start} + kT$, where $t_{start}$ is chosen after the transient phase within the test window. The resulting point clouds $P_{pred}$ and $P_{true}$ are compared using Maximum Mean Discrepancy (MMD) with a Gaussian RBF kernel, where the kernel bandwidth is selected via the median heuristic.

2. Fidelity is assessed by comparing the Power Spectral Density (PSD) of $x(t)$ over the test window. The PSD is computed using Welch's method with a Hanning window. A log-spectrum distance is used to verify the preservation of broadband frequency content.

3. The chaotic nature of the reconstruction is quantified by estimating the Largest Lyapunov Exponent (LLE) using the Rosenstein algorithm. The embedding dimension and delay are selected via False Nearest Neighbors and Mutual Information, respectively. Short-horizon RMSE is reported only as a secondary consistency check, given the inherent phase sensitivity of forced chaotic systems.

## Experimental Design and Success Criteria
Experiments are conducted across two observation regimes with different sparsity and noise levels, two integration horizons, and three random seeds. The methodology is considered successful if the balance-constrained PINN (M2) consistently improves attractor geometry (MMD), spectral fidelity (PSD), and Lyapunov estimates relative to the standard PINN (M0) and the conservation-regularized model (M1) in the sparse, noisy, position-only setting.
